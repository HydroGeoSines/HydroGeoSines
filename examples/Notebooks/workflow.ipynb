{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charged-administrator",
   "metadata": {},
   "source": [
    "# HydroGeoSines\n",
    "\n",
    "## A general data processing workflow\n",
    "\n",
    "This notebook demonstrates the general data handling capabilities of HydroGeoSines. The standard workflow for loading, processing and analysing data, as well as exporting and visualizing results is demonstrated on a simple example dataset. We show how the Site object and its methods can be used to store data and how the data processing is handled via the Processing object and its methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-andrews",
   "metadata": {},
   "source": [
    "### Import HGS\n",
    "Currently, the HydroGeoSines is not fully implemented as an installable package. Instead. we have to move to the parent directory, to import the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excellent-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  D:\\Workspaces\\GitHub\\HydroGeoSines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../../\")\n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "\n",
    "# Load the HGS package\n",
    "import hydrogeosines as hgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reverse-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and other packages used in this tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-valuation",
   "metadata": {},
   "source": [
    "### The Site object\n",
    "Typically, we have time series data of groundwater head measurements from a couple of different loggers that are located at a site of interest. Similarly, we aggreate all our data records into a hgs.Site object. The Site object has a geo-location that attribute to add information on longitude, latitude and height . This is can later be used to calculate site specific Earth Tide records.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infectious-stylus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hydrogeosines.models.site.Site object at 0x0000026438D51E88>\n"
     ]
    }
   ],
   "source": [
    "# Create a Site object\n",
    "example_site = hgs.Site('example', geoloc=[141.762065, -31.065781, 160])\n",
    "print(example_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-liberal",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "#### Groundwater head records\n",
    "The import_csv method of the Site object can be used to import the three standard input categories \"GW\", \"BP\" and \"ET\" (groundwater, barometric pressure, and earth tides). In general, the hgs package is implemented in SI units. By passing a *unit* argument for your input dataset, units are automatically converted. \n",
    "\n",
    "In the present example, a dataset with three groundwater records is loaded. The location names are explicitly set as \"Loc_A\", \"Loc_B\" and \"Loc_C\" using the loc_names parameter, because there are no column headers in the data set (header = None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "included-attack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new time series was added ...\n",
      "No dublicates being found ...\n"
     ]
    }
   ],
   "source": [
    "# Load all our data attributed to the Site\n",
    "example_site.import_csv('tests/data/notebook/GW_record.csv', \n",
    "                        input_category=[\"GW\"]*3, \n",
    "                        utc_offset=10, unit=[\"m\"]*3,\n",
    "                        loc_names = [\"Loc_A\",\"Loc_B\",\"Loc_C\"], header = None,\n",
    "                        check_dublicates=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-pharmacology",
   "metadata": {},
   "source": [
    "The Site object now has the groundwater records added to its data attribute. It is stored as a Pandas DataFrame with a set of predefined column names:\n",
    " - **datetime:** the first column of every input data record should be a datetime convertible format\n",
    " - **category:** the data category (GW,BP or ET)\n",
    " - **location:** either infered from the header or defined by the loc_names parameter of the import method\n",
    " - **part:** pre-set to \"all\". For non-uniform data records, the data set is later split into uniform parts\n",
    " - **unit:** unit (SI after import)\n",
    " - **value** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "impaired-breathing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "      <th>location</th>\n",
       "      <th>part</th>\n",
       "      <th>unit</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-12-31 14:00:30+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>all</td>\n",
       "      <td>m</td>\n",
       "      <td>7.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-12-31 14:05:30+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>all</td>\n",
       "      <td>m</td>\n",
       "      <td>7.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-12-31 14:10:30+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>all</td>\n",
       "      <td>m</td>\n",
       "      <td>7.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime category location part unit  value\n",
       "0 2000-12-31 14:00:30+00:00       GW    Loc_A  all    m  7.017\n",
       "1 2000-12-31 14:05:30+00:00       GW    Loc_A  all    m  7.017\n",
       "2 2000-12-31 14:10:30+00:00       GW    Loc_A  all    m  7.016"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_site.data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-tension",
   "metadata": {},
   "source": [
    "#### Barometric pressure records\n",
    "The import of barometric pressure records is similar to the groundwater head import. Only \"BP\" needs to be passed as an argument to the \"category\" parameter. Setting the *how* parameter to \"all\", the Site data attribute is updated and the BP record is added to the previously imported GW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "republican-specialist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new time series was added ...\n",
      "No dublicates being found ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example_site.import_csv('tests/data/notebook/BP_record.csv', \n",
    "                        input_category=\"BP\", \n",
    "                        utc_offset=10, unit=\"m\", \n",
    "                        loc_names = \"Baro\",\n",
    "                        header = None,\n",
    "                        how=\"add\", check_dublicates=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vertical-irrigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 174973 entries, 0 to 174972\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count   Dtype              \n",
      "---  ------    --------------   -----              \n",
      " 0   datetime  174973 non-null  datetime64[ns, UTC]\n",
      " 1   category  174973 non-null  object             \n",
      " 2   location  174973 non-null  object             \n",
      " 3   part      174973 non-null  object             \n",
      " 4   unit      174973 non-null  object             \n",
      " 5   value     87699 non-null   float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), object(4)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "example_site.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-lewis",
   "metadata": {},
   "source": [
    "### The Processing object\n",
    "The Processing object enables easy access to the hgs methods for data pre-processing and data analysis. These include methods for calculating barometric efficiencies, corrected groundwater heads or extracting harmonic components from records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "patent-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Processing object of example site\n",
    "process_example = hgs.Processing(example_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-enclosure",
   "metadata": {},
   "source": [
    "After instantiating the Processing object, we can simply run the desired method, which returns a new object containing the method results. In this case, we want to compute all available time domain barometric efficiencies (BE) available in the BE_time method. \n",
    "\n",
    "The BE_time methods requires our data to be uniformly sampled. Thus, preprocessing steps are applied to the data of the Site object. First the groundwater head measurements are resampled, interpolated and if necessary split into sub-parts of uniform sampling. Then the BP records are aligned with the GW data. Then the barometric efficiencies are calculated for every location and part individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "veterinary-store",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing BE_time method...\n",
      "9.84 % of the 'GW' data at 'Loc_A_all' was interpolated due to gaps < 3600s!\n",
      "9.67 % of the 'GW' data at 'Loc_B_all' was interpolated due to gaps < 3600s!\n",
      "9.76 % of the 'GW' data at 'Loc_C_all' was interpolated due to gaps < 3600s!\n",
      "Data of the category 'GW' is regularly sampled now!\n",
      "6.26 % of the 'BP' data at 'Baro_all' was interpolated due to gaps < 3600s!\n",
      "0.00 % of the 'GW' data at 'Loc_A_1' was interpolated due to gaps < 3600s!\n",
      "0.00 % of the 'GW' data at 'Loc_B_1' was interpolated due to gaps < 3600s!\n",
      "0.00 % of the 'GW' data at 'Loc_C_1' was interpolated due to gaps < 3600s!\n",
      "0.00 % of the 'GW' data at 'Loc_C_2' was interpolated due to gaps < 3600s!\n",
      "The groundwater (GW) and barometric pressure (BP) data is now aligned. There is now exactly one BP for every GW entry!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_A', '1')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_A', '2')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_B', '1')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_B', '2')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_C', '1')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_C', '2')'!\n"
     ]
    }
   ],
   "source": [
    "# Test the BE Time methods\n",
    "BE_results  = process_example.BE_time(method=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-activation",
   "metadata": {},
   "source": [
    "BE_results now contains a nested dictionary for the BE_time method results. The dictionary labels correspond to the name of the location, its sub-parts and the individual BE methods used on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complete-brook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'be_time': {'Loc_A': {'2': {'BP': array([-0.00101974, -0.10095468,  0.10503366, ...,  0.        ,\n",
      "       -0.00305923,  0.        ]), 'GW': array([-0.001,  0.004, -0.004, ..., -0.001, -0.003,  0.006]), 'dt': 7064    2001-03-01 07:20:00+00:00\n",
      "7065    2001-03-01 07:25:00+00:00\n",
      "7066    2001-03-01 07:30:00+00:00\n",
      "7067    2001-03-01 07:35:00+00:00\n",
      "7068    2001-03-01 07:40:00+00:00\n",
      "                   ...           \n",
      "15491   2001-03-30 13:35:00+00:00\n",
      "15492   2001-03-30 13:40:00+00:00\n",
      "15493   2001-03-30 13:45:00+00:00\n",
      "15494   2001-03-30 13:50:00+00:00\n",
      "15495   2001-03-30 13:55:00+00:00\n",
      "Name: datetime, Length: 8432, dtype: datetime64[ns, UTC], 'derivative': True, 'clark': 0.10214224288026245, 'davis_and_rasmussen': -0.24966269348758294, 'rahi': 0.3529139626245524, 'rojstaczer': 0.5039950805827401, 'average_of_ratios': 0.068208979341703, 'linear_regression': 0.005787699199442982, 'median_of_ratios': 0.0}}, 'Loc_B': {'2': {'BP': array([-0.00101974, -0.10095468,  0.10503366, ...,  0.        ,\n",
      "       -0.00305923,  0.        ]), 'GW': array([-0.001, -0.001,  0.001, ..., -0.002, -0.002,  0.002]), 'dt': 27130   2001-03-01 07:20:00+00:00\n",
      "27131   2001-03-01 07:25:00+00:00\n",
      "27132   2001-03-01 07:30:00+00:00\n",
      "27133   2001-03-01 07:35:00+00:00\n",
      "27134   2001-03-01 07:40:00+00:00\n",
      "                   ...           \n",
      "35557   2001-03-30 13:35:00+00:00\n",
      "35558   2001-03-30 13:40:00+00:00\n",
      "35559   2001-03-30 13:45:00+00:00\n",
      "35560   2001-03-30 13:50:00+00:00\n",
      "35561   2001-03-30 13:55:00+00:00\n",
      "Name: datetime, Length: 8432, dtype: datetime64[ns, UTC], 'derivative': True, 'clark': 0.1326289849004394, 'davis_and_rasmussen': -0.2590200108635409, 'rahi': 0.34483211903692473, 'rojstaczer': 0.4398811523252397, 'average_of_ratios': 0.08904743261804568, 'linear_regression': 0.022386230719819388, 'median_of_ratios': 0.0}}, 'Loc_C': {'2': {'BP': array([-0.00101974, -0.10095468,  0.10503366, ...,  0.        ,\n",
      "       -0.00305923,  0.        ]), 'GW': array([ 0.   ,  0.001, -0.003, ..., -0.004, -0.001,  0.004]), 'dt': 48414   2001-03-01 07:20:00+00:00\n",
      "48415   2001-03-01 07:25:00+00:00\n",
      "48416   2001-03-01 07:30:00+00:00\n",
      "48417   2001-03-01 07:35:00+00:00\n",
      "48418   2001-03-01 07:40:00+00:00\n",
      "                   ...           \n",
      "56841   2001-03-30 13:35:00+00:00\n",
      "56842   2001-03-30 13:40:00+00:00\n",
      "56843   2001-03-30 13:45:00+00:00\n",
      "56844   2001-03-30 13:50:00+00:00\n",
      "56845   2001-03-30 13:55:00+00:00\n",
      "Name: datetime, Length: 8432, dtype: datetime64[ns, UTC], 'derivative': True, 'clark': 0.11276381191160476, 'davis_and_rasmussen': -0.25013846393440275, 'rahi': 0.35000840030368197, 'rojstaczer': 0.46420971486297424, 'average_of_ratios': 0.06646641261708976, 'linear_regression': 0.0444752199784296, 'median_of_ratios': 0.0}}}}\n"
     ]
    }
   ],
   "source": [
    "print(BE_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-kitchen",
   "metadata": {},
   "source": [
    "#### Filter by groundwater location\n",
    "Once we created our Site object containing all our data, we can decide to process certain locations individually, using the gw_loc method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alleged-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Processing object for a specific groundwater location of example_site\n",
    "locations = \"Loc_A\"\n",
    "process_loc_A = hgs.Processing(example_site).by_gwloc(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-sheep",
   "metadata": {},
   "source": [
    "#### Add regular data attribute to the processing object\n",
    "BE_time and other methods require the data to be uniformly sampled. Thus, if multiply multiple methods need access to uniformly sampled data it sometimes makes sense to pre-process the data using the make_regular() method to reduce the overall processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bizarre-heritage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.84 % of the 'GW' data at 'Loc_A_all' was interpolated due to gaps < 3600s!\n",
      "9.67 % of the 'GW' data at 'Loc_B_all' was interpolated due to gaps < 3600s!\n",
      "Data of the category 'GW' is regularly sampled now!\n",
      "0.02 % of the 'BP' data at 'Baro_all' was interpolated due to gaps < 3600s!\n",
      "0.00 % of the 'GW' data at 'Loc_A_1' was interpolated due to gaps < 3600s!\n",
      "0.00 % of the 'GW' data at 'Loc_B_1' was interpolated due to gaps < 3600s!\n",
      "The groundwater (GW) and barometric pressure (BP) data is now aligned. There is now exactly one BP for every GW entry!\n"
     ]
    }
   ],
   "source": [
    "# Create Processing object for two groundwater location of example_site and add a regularly sampled data attribute.\n",
    "# It is automatically reused in some of the methods, reducing computation times\n",
    "locations = [\"Loc_A\",\"Loc_B\"]\n",
    "process_loc_AB = hgs.Processing(example_site).by_gwloc(locations).make_regular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exciting-grammar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing BE_time method...\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_A', '1')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_A', '2')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_B', '1')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_B', '2')'!\n"
     ]
    }
   ],
   "source": [
    "be_results_2 = process_loc_AB.BE_time(method=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-commerce",
   "metadata": {},
   "source": [
    "### The View object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
