{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charged-administrator",
   "metadata": {},
   "source": [
    "# HydroGeoSines\n",
    "\n",
    "## A general data processing workflow\n",
    "\n",
    "This notebook demonstrates the general data handling capabilities of HydroGeoSines. The standard workflow for loading, processing and analysing data, as well as exporting and visualizing results is demonstrated on a simple example dataset. We show how the Site object and its methods can be used to store data and how the data processing is handled via the Processing object and its methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-andrews",
   "metadata": {},
   "source": [
    "### Import HGS\n",
    "Currently, the HydroGeoSines is not fully implemented as an installable package. Instead. we have to move to the parent directory, to import the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excellent-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  /media/daniel/SharedData/Workspaces/GitHub/HydroGeoSines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../../\")\n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "\n",
    "# Load the HGS package\n",
    "import hydrogeosines as hgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reverse-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and other packages used in this tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-valuation",
   "metadata": {},
   "source": [
    "### The Site object\n",
    "Typically, we have time series data of groundwater head measurements from a couple of different loggers that are located at a site of interest. Similarly, we aggreate all our data records into a hgs.Site object. The Site object has a geo-location that attribute to add information on longitude, latitude and height . This is can later be used to calculate site specific Earth Tide records.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infectious-stylus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hydrogeosines.models.site.Site object at 0x7fc584401970>\n"
     ]
    }
   ],
   "source": [
    "# Create a Site object\n",
    "example_site = hgs.Site('example', geoloc=[141.762065, -31.065781, 160])\n",
    "print(example_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-liberal",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "#### Groundwater head records\n",
    "The import_csv method of the Site object can be used to import the three standard input categories \"GW\", \"BP\" and \"ET\" (groundwater, barometric pressure, and earth tides). In general, the hgs package is implemented in SI units. By passing a *unit* argument for your input dataset, units are automatically converted. \n",
    "\n",
    "In the present example, a dataset with three groundwater records is loaded. The location names are explicitly set as \"Loc_A\", \"Loc_B\" and \"Loc_C\" using the loc_names parameter, because there are no column headers in the data set (header = None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "included-attack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new time series was added ...\n",
      "No duplicates found ...\n"
     ]
    }
   ],
   "source": [
    "# Load all our data attributed to the Site\n",
    "example_site.import_csv('tests/data/notebook/GW_record.csv', \n",
    "                        input_category=[\"GW\"]*3, \n",
    "                        utc_offset=10, unit=[\"m\"]*3,\n",
    "                        loc_names = [\"Loc_A\",\"Loc_B\",\"Loc_C\"], header = None,\n",
    "                        check_duplicates=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-pharmacology",
   "metadata": {},
   "source": [
    "The Site object now has the groundwater records added to its data attribute. It is stored as a Pandas DataFrame with a set of predefined column names:\n",
    " - **datetime:** the first column of every input data record should be a datetime convertible format\n",
    " - **category:** the data category (GW,BP or ET)\n",
    " - **location:** either infered from the header or defined by the loc_names parameter of the import method\n",
    " - **part:** pre-set to \"all\". For non-uniform data records, the data set is later split into uniform parts\n",
    " - **unit:** unit (SI after import)\n",
    " - **value** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "impaired-breathing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "      <th>location</th>\n",
       "      <th>part</th>\n",
       "      <th>unit</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-12-31 14:00:30+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>all</td>\n",
       "      <td>m</td>\n",
       "      <td>7.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-12-31 14:05:30+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>all</td>\n",
       "      <td>m</td>\n",
       "      <td>7.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-12-31 14:10:30+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>all</td>\n",
       "      <td>m</td>\n",
       "      <td>7.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime category location part unit  value\n",
       "0 2000-12-31 14:00:30+00:00       GW    Loc_A  all    m  7.017\n",
       "1 2000-12-31 14:05:30+00:00       GW    Loc_A  all    m  7.017\n",
       "2 2000-12-31 14:10:30+00:00       GW    Loc_A  all    m  7.016"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_site.data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-tension",
   "metadata": {},
   "source": [
    "#### Barometric pressure records\n",
    "The import of barometric pressure records is similar to the groundwater head import. Only \"BP\" needs to be passed as an argument to the \"category\" parameter. Setting the *how* parameter to \"all\", the Site data attribute is updated and the BP record is added to the previously imported GW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "republican-specialist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new time series was added ...\n",
      "No duplicates found ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example_site.import_csv('tests/data/notebook/BP_record.csv', \n",
    "                        input_category=\"BP\", \n",
    "                        utc_offset=10, unit=\"m\", \n",
    "                        loc_names = \"Baro\",\n",
    "                        header = None,\n",
    "                        how=\"add\", check_duplicates=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vertical-irrigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 174973 entries, 0 to 174972\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count   Dtype              \n",
      "---  ------    --------------   -----              \n",
      " 0   datetime  174973 non-null  datetime64[ns, UTC]\n",
      " 1   category  174973 non-null  object             \n",
      " 2   location  174973 non-null  object             \n",
      " 3   part      174973 non-null  object             \n",
      " 4   unit      174973 non-null  object             \n",
      " 5   value     87699 non-null   float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), object(4)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "example_site.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-lewis",
   "metadata": {},
   "source": [
    "### The Processing object\n",
    "The Processing object enables easy access to the hgs methods for data pre-processing and data analysis. These include methods for calculating barometric efficiencies, corrected groundwater heads or extracting harmonic components from records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "patent-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Processing object of example site\n",
    "process_example = hgs.Processing(example_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-enclosure",
   "metadata": {},
   "source": [
    "After instantiating the Processing object, we can simply run the desired method, which returns a new object containing the method results. In this case, we want to compute all available time domain barometric efficiencies (BE) available in the BE_time method. \n",
    "\n",
    "The BE_time methods requires our data to be uniformly sampled. Thus, preprocessing steps are applied to the data of the Site object. First the groundwater head measurements are resampled, interpolated and if necessary split into sub-parts of uniform sampling. Then the BP records are aligned with the GW data. Then the barometric efficiencies are calculated for every location and part individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "veterinary-store",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BE_time method...\n",
      "-------------------------------------------------\n",
      "{}\n",
      "9.84 % of the 'GW' data at 'Loc_A_all' was interpolated due to gaps < 3600s!\n",
      "9.67 % of the 'GW' data at 'Loc_B_all' was interpolated due to gaps < 3600s!\n",
      "9.76 % of the 'GW' data at 'Loc_C_all' was interpolated due to gaps < 3600s!\n",
      "Data of the category 'GW' is regularly sampled now!\n",
      "6.26 % of the 'BP' data at 'Baro_all' was interpolated due to gaps < 3600s!\n",
      "0.00 % of the 'GW' data at 'Loc_A_1' was interpolated due to gaps < 3600s!\n",
      "0.00 % of the 'GW' data at 'Loc_B_1' was interpolated due to gaps < 3600s!\n",
      "0.00 % of the 'GW' data at 'Loc_C_1' was interpolated due to gaps < 3600s!\n",
      "0.00 % of the 'GW' data at 'Loc_C_2' was interpolated due to gaps < 3600s!\n",
      "The groundwater (GW) and  BP data is aligned. There is exactly one BP for every GW entry!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_A', '1')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_A', '2')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_B', '1')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_B', '2')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_C', '1')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_C', '2')'!\n"
     ]
    }
   ],
   "source": [
    "# Test the BE Time methods\n",
    "BE_results  = process_example.BE_time(method=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-activation",
   "metadata": {},
   "source": [
    "#### The results container\n",
    "BE_results now contains a <font color=\"red\">nested</font> dictionary for the BE_time method results. The top level of the nested dictionary constains one item for each method that has been applied to the data. The second level contains one item for each location and its sub-parts.\n",
    "\n",
    "- Each method is stored as an item in the results dictionary with the name of the method as the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f857b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['be_time'])\n"
     ]
    }
   ],
   "source": [
    "print(BE_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d75d7e",
   "metadata": {},
   "source": [
    "- The method dictionary items are also dictionaries (i.e. forming a nested dictionary). For each location a seperate entry is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341945c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('Loc_A', '1'), ('Loc_A', '2'), ('Loc_B', '1'), ('Loc_B', '2'), ('Loc_C', '1'), ('Loc_C', '2')])\n"
     ]
    }
   ],
   "source": [
    "print(BE_results[\"be_time\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb04c5",
   "metadata": {},
   "source": [
    " - The final method results are stored as a list with 3 entries. The first entry contains the method output (0), the second the input data (1) and the third entry is for additional information (2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "complete-brook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " {'clark': 0.12311537803698128, 'davis_and_rasmussen': -0.258738991944098, 'rahi': 0.34126467174424036, 'rojstaczer': 0.6487539850861026, 'average_of_ratios': 0.011837536692833276, 'linear_regression': 0.012507017832631329, 'median_of_ratios': 0.0} \n",
      "\n",
      "Input:\n",
      "                               GW        BP\n",
      "datetime                                  \n",
      "2001-02-03 03:55:00+00:00  0.001 -0.004079\n",
      "2001-02-03 04:00:00+00:00  0.003 -0.002039\n",
      "2001-02-03 04:05:00+00:00 -0.001  0.000000\n",
      "2001-02-03 04:10:00+00:00 -0.002  0.002039\n",
      "2001-02-03 04:15:00+00:00  0.001 -0.003059\n",
      "...                          ...       ...\n",
      "2001-02-27 16:05:00+00:00  0.000  0.000000\n",
      "2001-02-27 16:10:00+00:00 -0.003  0.000000\n",
      "2001-02-27 16:15:00+00:00  0.000 -0.003059\n",
      "2001-02-27 16:20:00+00:00  0.004  0.003059\n",
      "2001-02-27 16:25:00+00:00 -0.004 -0.002039\n",
      "\n",
      "[7063 rows x 2 columns] \n",
      "\n",
      "Info:\n",
      " {'derivative': True, 'unit': '-', 'utc_offset': 10} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\",BE_results[\"be_time\"][\"Loc_A\",\"1\"][0],\"\\n\")\n",
    "print(\"Input:\\n\", BE_results[\"be_time\"][\"Loc_A\",\"1\"][1],\"\\n\")\n",
    "print(\"Info:\\n\", BE_results[\"be_time\"][\"Loc_A\",\"1\"][2],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-kitchen",
   "metadata": {},
   "source": [
    "#### Filter by groundwater location\n",
    "Once we created our Site object containing all our data, we can decide to process only a subset of the available locations, using the gw_loc method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alleged-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Processing object for two specific groundwater location of example_site\n",
    "locations = [\"Loc_A\", \"Loc_B\"]\n",
    "process_loc_AB = hgs.Processing(example_site).by_gwloc(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-sheep",
   "metadata": {},
   "source": [
    "#### Add regular data attribute to the processing object\n",
    "BE_time and other methods require the data to be uniformly sampled. Thus, if multiply multiple methods need access to uniformly sampled data it sometimes makes sense to pre-process the data using the make_regular() method to reduce the overall processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bizarre-heritage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inter_max': 5000, 'inter_max_total': 40}\n",
      "9.84 % of the 'GW' data at 'Loc_A_all' was interpolated due to gaps < 5000s!\n",
      "Not enough data for 'Loc_A' to ensure minimum part size!\n",
      "9.67 % of the 'GW' data at 'Loc_B_all' was interpolated due to gaps < 5000s!\n",
      "Data of the category 'GW' is regularly sampled now!\n",
      "0.02 % of the 'BP' data at 'Baro_all' was interpolated due to gaps < 5000s!\n",
      "0.00 % of the 'GW' data at 'Loc_B_1' was interpolated due to gaps < 5000s!\n",
      "The groundwater (GW) and  BP data is aligned. There is exactly one BP for every GW entry!\n"
     ]
    }
   ],
   "source": [
    "process_MR = process_loc_AB.RegularAndAligned(inter_max=5000, part_min=60,inter_max_total=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "exciting-grammar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BE_time method...\n",
      "-------------------------------------------------\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_B', '1')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_B', '2')'!\n"
     ]
    }
   ],
   "source": [
    "be_results_2 = process_loc_AB.BE_time(method=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-commerce",
   "metadata": {},
   "source": [
    "### The View object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2443130f",
   "metadata": {},
   "source": [
    "... under preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68758aa4",
   "metadata": {},
   "source": [
    "### Advanced and manual preprocessing\n",
    "Although the Processing class automatically handles and applies all required and recommended preprocessing steps for the analysis methods to work, these can also be set by the user. \n",
    "\n",
    "####  The make_regular() method\n",
    "The preprocess_data() method consists of two main functions, one to make the groundwater data regularly sampled and a second one to align the BP entries to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6baa6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fa716c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "sig = inspect.signature(process_loc_AB.data.hgs.BP_align)\n",
    "if \"inter_max\" in sig.parameters.keys():\n",
    "    print(\"YES\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
