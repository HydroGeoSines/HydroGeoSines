{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charged-administrator",
   "metadata": {},
   "source": [
    "# HydroGeoSines\n",
    "## A general data processing workflow\n",
    "\n",
    "This notebook demonstrates the general data handling capabilities of HydroGeoSines. The standard workflow for loading, processing and analysing data, as well as exporting and visualizing results is demonstrated on a simple example dataset. We show how the Site object and its methods can be used to store data and how the data processing is handled via the Processing object and its methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-andrews",
   "metadata": {},
   "source": [
    "### Import HGS\n",
    "Currently, the HydroGeoSines is not fully implemented as an installable package. Instead. we have to move to the parent directory, to import the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excellent-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  /media/daniel/SharedData/Workspaces/GitHub/HydroGeoSines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../../\")\n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "\n",
    "# Load the HGS package\n",
    "import hydrogeosines as hgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reverse-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and other packages used in this tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-valuation",
   "metadata": {},
   "source": [
    "### The Site object\n",
    "Typically, we have time series data of groundwater head measurements from a couple of different loggers that are located at a site of interest. Similarly, we aggreate all our data records into a hgs.Site object. The Site object has a geo-location that attribute to add information on longitude, latitude and height . This is can later be used to calculate site specific Earth Tide records.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infectious-stylus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hydrogeosines.models.site.Site object at 0x7ff74d517be0>\n"
     ]
    }
   ],
   "source": [
    "# Create a Site object\n",
    "example_site = hgs.Site('example', geoloc=[141.762065, -31.065781, 160])\n",
    "print(example_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-liberal",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "#### Import groundwater head records\n",
    "The import_csv method of the Site object can be used to import the three standard input categories \"GW\", \"BP\" and \"ET\" (groundwater, barometric pressure, and earth tides). In general, the hgs package is implemented in SI units. By passing a *unit* argument for your input dataset, units are automatically converted. \n",
    "\n",
    "In the present example, a dataset with three groundwater records is loaded. The location names are explicitly set as \"Loc_A\", \"Loc_B\" and \"Loc_C\" using the loc_names parameter, because there are no column headers in the data set (header = None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "included-attack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new time series was added ...\n",
      "No duplicate entries were found.\n"
     ]
    }
   ],
   "source": [
    "# Load all our data attributed to the Site\n",
    "example_site.import_csv('tests/data/notebook/GW_record.csv', \n",
    "                        input_category=[\"GW\"]*3, \n",
    "                        utc_offset=10, \n",
    "                        unit=[\"m\"]*3,\n",
    "                        loc_names = [\"Loc_A\",\"Loc_B\"], \n",
    "                        header = None,\n",
    "                        check_duplicates=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-pharmacology",
   "metadata": {},
   "source": [
    "The Site object now has the groundwater records added to its data attribute. It is stored as a Pandas DataFrame with a set of predefined column names:\n",
    " - **datetime:** the first column of every input data record should be a datetime convertible format\n",
    " - **category:** the data category (GW,BP or ET)\n",
    " - **location:** either infered from the header or defined by the loc_names parameter of the import method\n",
    " - **part:** pre-set to \"all\". For non-uniform data records, the data set is later split into uniform parts\n",
    " - **unit:** unit (SI after import)\n",
    " - **value** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "impaired-breathing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "      <th>location</th>\n",
       "      <th>part</th>\n",
       "      <th>unit</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-12-31 14:00:30+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>all</td>\n",
       "      <td>m</td>\n",
       "      <td>7.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-12-31 14:05:30+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>all</td>\n",
       "      <td>m</td>\n",
       "      <td>7.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-12-31 14:10:30+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>all</td>\n",
       "      <td>m</td>\n",
       "      <td>7.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime category location part unit  value\n",
       "0 2000-12-31 14:00:30+00:00       GW    Loc_A  all    m  7.017\n",
       "1 2000-12-31 14:05:30+00:00       GW    Loc_A  all    m  7.017\n",
       "2 2000-12-31 14:10:30+00:00       GW    Loc_A  all    m  7.016"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_site.data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed1bfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Loc_A', 'Loc_B'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_site.data.location.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-tension",
   "metadata": {},
   "source": [
    "#### Import barometric pressure records\n",
    "The import of barometric pressure records is similar to the groundwater head import. Only \"BP\" needs to be passed as an argument to the \"category\" parameter. Setting the *how* parameter to \"all\", the Site data attribute is updated and the BP record is added to the previously imported GW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "republican-specialist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new time series was added ...\n",
      "No duplicate entries were found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example_site.import_csv('tests/data/notebook/BP_record.csv', \n",
    "                        input_category=\"BP\", \n",
    "                        utc_offset=10, \n",
    "                        unit=\"m\", \n",
    "                        loc_names = \"Baro\",\n",
    "                        header = None,\n",
    "                        how=\"add\", check_duplicates=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vertical-irrigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108063 entries, 0 to 108062\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count   Dtype              \n",
      "---  ------    --------------   -----              \n",
      " 0   datetime  108063 non-null  datetime64[ns, UTC]\n",
      " 1   category  108063 non-null  object             \n",
      " 2   location  108063 non-null  object             \n",
      " 3   part      108063 non-null  object             \n",
      " 4   unit      108063 non-null  object             \n",
      " 5   value     68173 non-null   float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), object(4)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "example_site.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-lewis",
   "metadata": {},
   "source": [
    "### The Processing object\n",
    "The Processing object enables easy access to the hgs methods for data pre-processing and data analysis. These include methods for calculating barometric efficiencies, corrected groundwater heads or extracting harmonic components from records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "patent-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Processing object of example site\n",
    "process_example = hgs.Processing(example_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f9a925",
   "metadata": {},
   "source": [
    "#### Get comprehensive info on the data loaded into the  processing object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25194f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Summary of dataset:\n",
      "-------------------------------------------------\n",
      "Category: GW, Location: Loc_A\n",
      "Start: 31/12/2000 14:00:30 UTC\n",
      "Stop:  30/03/2001 13:55:49 UTC\n",
      "UTC offset: +10.00 h\n",
      "Sampling: 280-84270 sec (irregular)\n",
      "Values: 43,810 (19,744 empty)\n",
      "Unit: m\n",
      "-------------------------------------------------\n",
      "Category: GW, Location: Loc_B\n",
      "Start: 31/12/2000 14:00:30 UTC\n",
      "Stop:  30/03/2001 13:55:49 UTC\n",
      "UTC offset: +10.00 h\n",
      "Sampling: 275-200400 sec (irregular)\n",
      "Values: 43,810 (20,146 empty)\n",
      "Unit: m\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Category: BP, Location: Baro\n",
      "Start: 31/12/2000 14:00:00 UTC\n",
      "Stop:  30/03/2001 13:59:59 UTC\n",
      "UTC offset: +10.00 h\n",
      "Sampling: 1-952200 sec (irregular)\n",
      "Values: 20,443 (0 empty)\n",
      "Unit: m\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "process_example.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-enclosure",
   "metadata": {},
   "source": [
    "After instantiating the Processing object, we can simply run the desired method, which returns a new object containing the method results. In this case, we want to compute all available time domain barometric efficiencies (BE) available in the BE_time() method. \n",
    "\n",
    "#### Example: Calculate BE using the BE_time() method\n",
    "The BE_time() methods requires our data to be uniformly sampled. Thus, preprocessing steps are applied to the data of the Site object. First the groundwater head measurements are resampled, interpolated and if necessary split into sub-parts of uniform sampling. Then the BP records are aligned with the GW data. Then the barometric efficiencies are calculated for every location and part individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "veterinary-store",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Processing BE_time method ...\n",
      "4.73 % of the 'GW' data at 'Loc_A_all' was interpolated due to gaps < 3600s!\n",
      "5.08 % of the 'GW' data at 'Loc_B_all' was interpolated due to gaps < 3600s!\n",
      "Data of the category 'GW' is regularly sampled now!\n",
      "\n",
      "Start iteration No. 1 ...\n",
      "\n",
      "----- Loc_A_1 -----\n",
      "BP record resampled to 1 sample per 300s.\n",
      "\n",
      "Processing BP gaps ...\n",
      "0.02 % of the 'BP' data at 'Baro_all' was interpolated due to gaps < 3600s!\n",
      "... gaps between 2001-02-27 16:30 and 2001-03-01 07:15 too large for interpolation.\n",
      "\n",
      "Processing GW gaps ...\n",
      "... dropping GW entries for which BP gaps are too big.\n",
      "0.00 % of the 'GW' data at 'Loc_A_1' was interpolated due to gaps < 3600s!\n",
      "\n",
      "----- Loc_B_1 -----\n",
      "BP record resampled to 1 sample per 300s.\n",
      "\n",
      "Processing BP gaps ...\n",
      "6.10 % of the 'BP' data at 'Baro_all' was interpolated due to gaps < 3600s!\n",
      "... gaps between 2001-02-27 16:30 and 2001-03-01 07:15 too large for interpolation.\n",
      "\n",
      "Processing GW gaps ...\n",
      "... dropping GW entries for which BP gaps are too big.\n",
      "0.00 % of the 'GW' data at 'Loc_B_1' was interpolated due to gaps < 3600s!\n",
      "\n",
      "Start iteration No. 2 ...\n",
      "\n",
      "----- Loc_A_1 -----\n",
      "... all done!\n",
      "\n",
      "----- Loc_A_2 -----\n",
      "... all done!\n",
      "\n",
      "----- Loc_B_1 -----\n",
      "... all done!\n",
      "\n",
      "----- Loc_B_2 -----\n",
      "... all done!\n",
      "The groundwater (GW) and  BP data is aligned. There is exactly one BP for every GW entry!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_A', '1')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_A', '2')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_B', '1')'!\n",
      "Successfully calculated using method 'all' on GW data from '('Loc_B', '2')'!\n"
     ]
    }
   ],
   "source": [
    "# Test the BE Time methods\n",
    "BE_results  = process_example.BE_time(method=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-activation",
   "metadata": {},
   "source": [
    "#### The results container\n",
    "BE_results now contains a <font color=\"red\">nested</font> dictionary for the BE_time method results. The top level of the nested dictionary constains one item for each method that has been applied to the data. The second level contains one item for each location and its sub-parts.\n",
    "\n",
    "- Each method is stored as an item in the results dictionary with the name of the method as the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f857b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['be_time'])\n"
     ]
    }
   ],
   "source": [
    "print(BE_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d75d7e",
   "metadata": {},
   "source": [
    "- The method dictionary items are also dictionaries (i.e. forming a nested dictionary). For each location a seperate entry is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "341945c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('Loc_A', '1'), ('Loc_A', '2'), ('Loc_B', '1'), ('Loc_B', '2')])\n"
     ]
    }
   ],
   "source": [
    "print(BE_results[\"be_time\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb04c5",
   "metadata": {},
   "source": [
    " - The final method results are stored as a list with 3 entries. The first entry (index 0) contains the method output, the second (1) the input data as a DataFrame with Datetime index, and the third entry (2) is for additional information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "complete-brook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " {'clark': 0.14785256869529242, 'davis_and_rasmussen': -0.28441191303176316, 'rahi': 0.38839574348629413, 'rojstaczer': 0.6783000137341513, 'average_of_ratios': 0.03110885146484908, 'linear_regression': 0.02210803548828672, 'median_of_ratios': 0.0} \n",
      "\n",
      "Input:\n",
      "                               GW        BP\n",
      "datetime                                  \n",
      "2001-02-03 03:50:00+00:00  0.010 -0.010197\n",
      "2001-02-03 03:55:00+00:00  0.001 -0.004079\n",
      "2001-02-03 04:00:00+00:00  0.003 -0.002039 \n",
      "\n",
      "Info:\n",
      " {'derivative': True, 'unit': '-', 'utc_offset': 10} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\",BE_results[\"be_time\"][\"Loc_A\",\"1\"][0],\"\\n\")\n",
    "print(\"Input:\\n\", BE_results[\"be_time\"][\"Loc_A\",\"1\"][1].head(3),\"\\n\")\n",
    "print(\"Info:\\n\", BE_results[\"be_time\"][\"Loc_A\",\"1\"][2],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-kitchen",
   "metadata": {},
   "source": [
    "#### How to filter data by groundwater location\n",
    "Once we created our Site object containing all our data, we can decide to process only a subset of the available locations, using the gw_loc method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alleged-label",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter dataset by location ...\n"
     ]
    }
   ],
   "source": [
    "# Create Processing object for only one specific groundwater location of example_site\n",
    "locations = [\"Loc_A\"]\n",
    "process_A = hgs.Processing(example_site).by_gwloc(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e0757",
   "metadata": {},
   "source": [
    "Lets check if there is now only the data of location A (\"Loc_A\") in our processing object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f4355df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Baro', 'Loc_A'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_A.site.data.location.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68758aa4",
   "metadata": {},
   "source": [
    "### Advanced and manual preprocessing\n",
    "Although the Processing class automatically handles and applies all required and recommended data preprocessing steps for the analysis methods to work, these can also be customized by the user. \n",
    "\n",
    "The RegularAndAligned() method consists of two main functions. First, the make_regular() method to regularly sample the groundwater data and second, the BP_align() method to align the BP entries to the groundwater data. As a result, every groundwater record will have a matching BP meassurement for the same point in time.\n",
    "\n",
    "####  The make_regular() method\n",
    "The make_regular() method can be accessed directly through the hgs pandas accessor:\n",
    "```python\n",
    "example_site.data.hgs.make_regular()\n",
    "```\n",
    "\n",
    "It has several parameters with default values:\n",
    " - **inter_max:** int = 3600 <br />*This is the maximum interpolated time interval in seconds. Any gap larger than this value will not be interpolated.*\n",
    " - **part_min:** int = 20 <br />*The minimum record duration without gaps in days. If there are gaps in the data that can not be interpolated, the data is split into parts. TIn this case, every part needs to fullfill the minimum criteria. Otherwise it is dropped from the data.* \n",
    " - **method:** str = \"backfill\" <br />*The interpolation method of Pandas to be used. Check out the Pandas documenation for more informations on the available methods.*\n",
    " - **category** = \"GW\" <br />*This method was developed for groundwater data, but can in principal be applied to other categories as well.*\n",
    " - **spl_freq:** int = None <br />*The method is automatically calculating the most common sampling frequency for each location. But the parameter can also be passed to the function as an argument.*\n",
    " - **inter_max_total:** int = 10 <br />*The maximum percentage threshold of values to be interpolated. If this threshold is exceeded, there were to many gaps in the data.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5996493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are missing values in the data: True\n"
     ]
    }
   ],
   "source": [
    "# select data from site object\n",
    "data = example_site.data\n",
    "# are there any nan in the value column of the groundwater category?\n",
    "print(\"There are missing values in the data:\", data[data.category == \"GW\"].value.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f6911",
   "metadata": {},
   "source": [
    "##### Upsample data\n",
    "Now lets upsample (i.e. interpolate the missing values) our data using the \"time\" method. Internally this calls on the following function, which is individually applied to all locations of the data:\n",
    "```python\n",
    "data.hgs.upsample(\"time\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a6baa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.73 % of the 'GW' data at 'Loc_A_all' was interpolated due to gaps < 3600s!\n",
      "5.08 % of the 'GW' data at 'Loc_B_all' was interpolated due to gaps < 3600s!\n",
      "Data of the category 'GW' is regularly sampled now!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "      <th>location</th>\n",
       "      <th>part</th>\n",
       "      <th>unit</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-02-03 03:45:00+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>1</td>\n",
       "      <td>m</td>\n",
       "      <td>6.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-03 03:50:00+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>1</td>\n",
       "      <td>m</td>\n",
       "      <td>6.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-02-03 03:55:00+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>1</td>\n",
       "      <td>m</td>\n",
       "      <td>6.723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime category location part unit  value\n",
       "0 2001-02-03 03:45:00+00:00       GW    Loc_A    1    m  6.712\n",
       "1 2001-02-03 03:50:00+00:00       GW    Loc_A    1    m  6.722\n",
       "2 2001-02-03 03:55:00+00:00       GW    Loc_A    1    m  6.723"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_resample = data.hgs.make_regular(method='time', inter_max = 3600)\n",
    "data_resample.hgs.filters.get_gw_data.head(3)\n",
    "\n",
    "#, part_min = 20, method = \"backfill\", category = \"GW\", spl_freq = None, inter_max_total = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16d3ad",
   "metadata": {},
   "source": [
    "##### Custom sampling frequency\n",
    "Resample data to a sampling frequency of 1 hour (3600 seconds).\n",
    "\n",
    "**Careful!** The interpolation maximum (inter_max) always has to be equal or higher than the sampling frequency. Otherwise your data won't be interpolated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2dea4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 % of the 'GW' data at 'Loc_A_all' was interpolated due to gaps < 5400s!\n",
      "0.00 % of the 'GW' data at 'Loc_B_all' was interpolated due to gaps < 5400s!\n",
      "Data of the category 'GW' is regularly sampled now!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "      <th>location</th>\n",
       "      <th>part</th>\n",
       "      <th>unit</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-02-03 03:00:00+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>1</td>\n",
       "      <td>m</td>\n",
       "      <td>6.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-03 04:00:00+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>1</td>\n",
       "      <td>m</td>\n",
       "      <td>6.726083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-02-03 05:00:00+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>1</td>\n",
       "      <td>m</td>\n",
       "      <td>6.729917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime category location part unit     value\n",
       "0 2001-02-03 03:00:00+00:00       GW    Loc_A    1    m  6.719000\n",
       "1 2001-02-03 04:00:00+00:00       GW    Loc_A    1    m  6.726083\n",
       "2 2001-02-03 05:00:00+00:00       GW    Loc_A    1    m  6.729917"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_resample = data.hgs.make_regular(inter_max = 5400, spl_freq = 3600)\n",
    "data_resample.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f43475",
   "metadata": {},
   "source": [
    "Our data has been resampled to one sample per hour. HGS also provides a DataFrame attribute to check for the most common sample frequency by group (i.e. splitted by category, location, parts and unit). This attribute is also accessed by the make_regular() method and used to configure the resampling.\n",
    "\n",
    "We can see that all GW data is sampled while the BP data has a sampling frequency of 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57ce3f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category  location  part  unit\n",
       "BP        Baro      all   m        300.0\n",
       "GW        Loc_A     1     m       3600.0\n",
       "          Loc_B     1     m       3600.0\n",
       "Name: datetime, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_resample.hgs.spl_freq_groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a37b83",
   "metadata": {},
   "source": [
    "####  The BP_align() method\n",
    "The BP_align() method can be accessed directly through the hgs pandas accessor:\n",
    "```python\n",
    "example_site.data.hgs.BP_align()\n",
    "```\n",
    "\n",
    "It has several parameters with default values, all of them can also be found in the make_regular() method:\n",
    "- **inter_max:** int = 3600\n",
    "- **method:** str = \"backfill\"\n",
    "- **part_min:** int = 20\n",
    "- **inter_max_total:** int = 10\n",
    "\n",
    "BP_align() automatically tries to match the sampling frequency of the groundwater records. It does so, by individually upsampling or downsampling the BP records for each GW location and its parts. Therefore, no user defined sampling frequency is available at this step. \n",
    "Gaps that exceed the inter_max threshold and thus, can not be interpolated are used to drop the according entries from the GW record. This generally causes another split into parts. The part_min parameter ensures that only parts large then the threshold are retained in the data.\n",
    "In some cases the BP and GW data can not be aligned. The main reason usually is that there are too many gaps in the BP record. In this case, try to reduce the part_min or increase the inter_max and inter_max_total parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5c09160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start iteration No. 1 ...\n",
      "\n",
      "----- Loc_A_1 -----\n",
      "BP record resampled to 1 sample per 3600s.\n",
      "\n",
      "Processing BP gaps ...\n",
      "0.00 % of the 'BP' data at 'Baro_all' was interpolated due to gaps < 5400s!\n",
      "... gaps between 2001-02-27 17:00 and 2001-03-01 06:00 too large for interpolation.\n",
      "\n",
      "Processing GW gaps ...\n",
      "... dropping GW entries for which BP gaps are too big.\n",
      "0.00 % of the 'GW' data at 'Loc_A_1' was interpolated due to gaps < 5400s!\n",
      "\n",
      "----- Loc_B_1 -----\n",
      "BP record resampled to 1 sample per 3600s.\n",
      "\n",
      "Processing BP gaps ...\n",
      "0.00 % of the 'BP' data at 'Baro_all' was interpolated due to gaps < 5400s!\n",
      "... gaps between 2001-02-27 17:00 and 2001-03-01 06:00 too large for interpolation.\n",
      "\n",
      "Processing GW gaps ...\n",
      "... dropping GW entries for which BP gaps are too big.\n",
      "0.00 % of the 'GW' data at 'Loc_B_1' was interpolated due to gaps < 5400s!\n",
      "\n",
      "Start iteration No. 2 ...\n",
      "\n",
      "----- Loc_A_1 -----\n",
      "... all done!\n",
      "\n",
      "----- Loc_A_2 -----\n",
      "... all done!\n",
      "\n",
      "----- Loc_B_1 -----\n",
      "... all done!\n",
      "\n",
      "----- Loc_B_2 -----\n",
      "... all done!\n"
     ]
    }
   ],
   "source": [
    "data_aligned = data_resample.hgs.BP_align(inter_max = 5400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d85d4",
   "metadata": {},
   "source": [
    "We can now check if the GW and BP data is truely aligned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfae305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The groundwater (GW) and  BP data is aligned. There is exactly one BP for every GW entry!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aligned.hgs.check_alignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b98d24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th>BP</th>\n",
       "      <th colspan=\"4\" halign=\"left\">GW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>Baro</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Loc_A</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Loc_B</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>part</th>\n",
       "      <th>all</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <th>m</th>\n",
       "      <th>m</th>\n",
       "      <th>m</th>\n",
       "      <th>m</th>\n",
       "      <th>m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-01-12 15:00:00+00:00</th>\n",
       "      <td>10.309615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.277000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-12 16:00:00+00:00</th>\n",
       "      <td>10.308085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.275500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-12 17:00:00+00:00</th>\n",
       "      <td>10.310635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.279500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-12 18:00:00+00:00</th>\n",
       "      <td>10.315733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.280417</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-12 19:00:00+00:00</th>\n",
       "      <td>10.319302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.281333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-03-30 09:00:00+00:00</th>\n",
       "      <td>10.655818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.955917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.260750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-03-30 10:00:00+00:00</th>\n",
       "      <td>10.660492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.957000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.261444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-03-30 11:00:00+00:00</th>\n",
       "      <td>10.661512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.955917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.260727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-03-30 12:00:00+00:00</th>\n",
       "      <td>10.658452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.956667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.260750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-03-30 13:00:00+00:00</th>\n",
       "      <td>10.658367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.954833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.260100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1809 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "category                          BP    GW                              \n",
       "location                        Baro Loc_A               Loc_B          \n",
       "part                             all     1         2         1         2\n",
       "unit                               m     m         m         m         m\n",
       "datetime                                                                \n",
       "2001-01-12 15:00:00+00:00  10.309615   NaN       NaN  1.277000       NaN\n",
       "2001-01-12 16:00:00+00:00  10.308085   NaN       NaN  1.275500       NaN\n",
       "2001-01-12 17:00:00+00:00  10.310635   NaN       NaN  1.279500       NaN\n",
       "2001-01-12 18:00:00+00:00  10.315733   NaN       NaN  1.280417       NaN\n",
       "2001-01-12 19:00:00+00:00  10.319302   NaN       NaN  1.281333       NaN\n",
       "...                              ...   ...       ...       ...       ...\n",
       "2001-03-30 09:00:00+00:00  10.655818   NaN  6.955917       NaN  1.260750\n",
       "2001-03-30 10:00:00+00:00  10.660492   NaN  6.957000       NaN  1.261444\n",
       "2001-03-30 11:00:00+00:00  10.661512   NaN  6.955917       NaN  1.260727\n",
       "2001-03-30 12:00:00+00:00  10.658452   NaN  6.956667       NaN  1.260750\n",
       "2001-03-30 13:00:00+00:00  10.658367   NaN  6.954833       NaN  1.260100\n",
       "\n",
       "[1809 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aligned.hgs.pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-sheep",
   "metadata": {},
   "source": [
    "#### Add the data_regular attribute to the processing object\n",
    "BE_time and other methods require the data to be uniformly sampled. Thus, if multiple methods need access to uniformly sampled data it sometimes makes sense to pre-process the data using the make_regular() method to reduce the overall processing time.\n",
    "\n",
    "You can also specify additional parameter that are internally passed to the make_regular() and BP_align() method. A comprehensive explanation for both methods and their parameters was given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bizarre-heritage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.73 % of the 'GW' data at 'Loc_A_all' was interpolated due to gaps < 5000s!\n",
      "5.08 % of the 'GW' data at 'Loc_B_all' was interpolated due to gaps < 5000s!\n",
      "Data of the category 'GW' is regularly sampled now!\n",
      "\n",
      "Start iteration No. 1 ...\n",
      "\n",
      "----- Loc_A_1 -----\n",
      "BP record resampled to 1 sample per 300s.\n",
      "\n",
      "Processing BP gaps ...\n",
      "0.02 % of the 'BP' data at 'Baro_all' was interpolated due to gaps < 5000s!\n",
      "... gaps between 2001-02-27 16:30 and 2001-03-01 07:15 too large for interpolation.\n",
      "\n",
      "Processing GW gaps ...\n",
      "... dropping GW entries for which BP gaps are too big.\n",
      "0.00 % of the 'GW' data at 'Loc_A_1' was interpolated due to gaps < 5000s!\n",
      "\n",
      "----- Loc_B_1 -----\n",
      "BP record resampled to 1 sample per 300s.\n",
      "\n",
      "Processing BP gaps ...\n",
      "6.10 % of the 'BP' data at 'Baro_all' was interpolated due to gaps < 5000s!\n",
      "... gaps between 2001-02-27 16:30 and 2001-03-01 07:15 too large for interpolation.\n",
      "\n",
      "Processing GW gaps ...\n",
      "... dropping GW entries for which BP gaps are too big.\n",
      "0.00 % of the 'GW' data at 'Loc_B_1' was interpolated due to gaps < 5000s!\n",
      "\n",
      "Start iteration No. 2 ...\n",
      "\n",
      "----- Loc_A_1 -----\n",
      "... all done!\n",
      "\n",
      "----- Loc_A_2 -----\n",
      "... all done!\n",
      "\n",
      "----- Loc_B_1 -----\n",
      "... all done!\n",
      "\n",
      "----- Loc_B_2 -----\n",
      "... all done!\n",
      "The groundwater (GW) and  BP data is aligned. There is exactly one BP for every GW entry!\n"
     ]
    }
   ],
   "source": [
    "# Create a Processing object of example site\n",
    "process_RAA = hgs.Processing(example_site).RegularAndAligned(inter_max=5000, part_min=20,inter_max_total=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685f78b",
   "metadata": {},
   "source": [
    "Now we have an attribute with the regularly sampled data added to our processing object which can be accessed manually and will automatically be used by processing methods such as BE_time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "decfb8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "      <th>location</th>\n",
       "      <th>part</th>\n",
       "      <th>unit</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-02-03 03:45:00+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>1</td>\n",
       "      <td>m</td>\n",
       "      <td>6.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-03 03:50:00+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>1</td>\n",
       "      <td>m</td>\n",
       "      <td>6.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-02-03 03:55:00+00:00</td>\n",
       "      <td>GW</td>\n",
       "      <td>Loc_A</td>\n",
       "      <td>1</td>\n",
       "      <td>m</td>\n",
       "      <td>6.723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime category location part unit  value\n",
       "0 2001-02-03 03:45:00+00:00       GW    Loc_A    1    m  6.712\n",
       "1 2001-02-03 03:50:00+00:00       GW    Loc_A    1    m  6.722\n",
       "2 2001-02-03 03:55:00+00:00       GW    Loc_A    1    m  6.723"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_RAA.data_regular.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6346c6fd",
   "metadata": {},
   "source": [
    "#### Compare runtime\n",
    "Time difference between running the BE_time with a precalculated data_regular() attribute using the RegularAndAligned method() and without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "exciting-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_example = hgs.Processing(example_site)\n",
    "# Turn off console output for readability\n",
    "with hgs.utils.nullify_output(suppress_stdout=True, suppress_stderr=True):\n",
    "    time1 = %timeit -n1 -r1 -o process_RAA.BE_time(method=\"all\")\n",
    "    time2 = %timeit -n1 -r1 -o process_example.BE_time(method=\"all\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cff8e588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With RAA: 684 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Without RAA: 2.52 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(\"With RAA:\",time1)\n",
    "print(\"Without RAA:\",time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-commerce",
   "metadata": {},
   "source": [
    "### The View object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2443130f",
   "metadata": {},
   "source": [
    "... under preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fa716c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "sig = inspect.signature(process_A.site.data.hgs.BP_align)\n",
    "if \"inter_max\" in sig.parameters.keys():\n",
    "    print(\"YES\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
